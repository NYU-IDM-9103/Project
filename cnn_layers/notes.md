Tried to visualize the feature maps from the different layers of a trained VGG model. I guess since VGG is trained on ImageNet dataset, which is well distributed across 1000 classes, the features maps almost show up as edge detection for a few intermediate layers. Even for the 2nd layer, the size of the feature map is quite small, so I attempted to upscale it using this [stable-diffusion-x3-upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler), but I couldn't get it to run even on Colab. I suspect it's because i didn't specify `pipeline.to("cuda")` but Colab T4 didn't seem to be able to run CUDA? I'll try some faster upscalers...

Next up, I'm planning to train a simple model from the faces dataset, so that I can try implementing a custom loss function. And then I'll visualize the feature maps from the layer(s) similar to what I've done and see how they look. I also wonder if I have a model trained only on human faces, even with a standard loss function, how would the layers look when I input a human face vs. a non-human face (say a building)?

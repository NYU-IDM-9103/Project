In order to test out my idea of having a custom loss function so that I can visualize how the layer(s) looked per training epoch, I tried this week to set up the code for a simple 2-layer NN trained on the faces dataset using a normal decreasing entropy loss function, and then visualize the first layer for subsequent epochs in the training loop. Since this was a very simple neural net trained on a very small sample data, I did not expect the feature maps of the first layer to show much details compared to visualizing the VGG16 model layers. Still, it was interesting to see how the first layer changed as the loss function decreased, it's like as if the model zoomed into specific pixels and "activated" those as the training progressed. I suppose if I were to reverse the loss function, then the first layer visualization would show increasingly more variation (or less focus), as the loss function increases. 
